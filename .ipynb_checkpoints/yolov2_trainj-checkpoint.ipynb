{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import chainer\n",
    "import glob\n",
    "import os\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "from chainer import serializers, optimizers, Variable, iterators,training, cuda\n",
    "from chainer.datasets import TransformDataset\n",
    "from chainer.training import extensions\n",
    "from chainer import Reporter, report, report_scope\n",
    "import chainer.functions as F\n",
    "from yolov2 import *\n",
    "from lib.utils import *\n",
    "from lib.preprocess import _offset_boxes, clip_boxes\n",
    "from transforms.transforms import *\n",
    "#from lib.image_generator import *\n",
    "from lib.data_generator import *\n",
    "from darknet19 import *\n",
    "from numpy.random import RandomState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_conv_layer(src, dst, layers):\n",
    "    for i in layers:\n",
    "        src_layer = eval(\"src.conv%d\" % i)\n",
    "        dst_layer = eval(\"dst.conv%d\" % i)\n",
    "        dst_layer.W = src_layer.W\n",
    "        dst_layer.b = src_layer.b\n",
    "\n",
    "def copy_bias_layer(src, dst, layers):\n",
    "    for i in layers:\n",
    "        src_layer = eval(\"src.bias%d\" % i)\n",
    "        dst_layer = eval(\"dst.bias%d\" % i)\n",
    "        dst_layer.b = src_layer.b\n",
    "\n",
    "def copy_bn_layer(src, dst, layers):\n",
    "    for i in layers:\n",
    "        src_layer = eval(\"src.bn%d\" % i)\n",
    "        dst_layer = eval(\"dst.bn%d\" % i)\n",
    "        dst_layer.N = src_layer.N\n",
    "        dst_layer.avg_var = src_layer.avg_var\n",
    "        dst_layer.avg_mean = src_layer.avg_mean\n",
    "        dst_layer.gamma = src_layer.gamma\n",
    "        dst_layer.eps = src_layer.eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert(batch, device):\n",
    "\n",
    "    #for pair in batch:\n",
    "        #print(pair[1])\n",
    "    return chainer.dataset.convert.concat_examples(batch, device, padding=0)\n",
    "\n",
    "def print_obs(t):\n",
    "    print(\"trainer.observation\", trainer.observation)\n",
    "    print(\"updater.loss\", updater.loss_func)\n",
    "    \n",
    "def save_model(t):\n",
    "    \n",
    "    serializers.save_hdf5(\"%s/yolov2_load.model\" % (\"./backup/result\"), model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "#train_sizes = [320, 352, 384, 416, 448]\n",
    "#train_sizes = [448]\n",
    "initial_weight_file = \"./backup/yolov2_load.model\"\n",
    "#initial_weight_file = None\n",
    "trained_weight_file = \"./darknet19_448.model\"\n",
    "snapshot=None\n",
    "#snapshot = \"./backup/result/snapshot_iter_4968\"\n",
    "backup_path = \"backup\"\n",
    "backup_file = \"%s/backup.model\" % (backup_path)\n",
    "batch_size =6\n",
    "epoch = 1000\n",
    "max_batches = 20000\n",
    "learning_rate = 1e-5\n",
    "learning_schedules = { \n",
    "    \"0\"    : 1e-5,\n",
    "    \"500\"  : 1e-4,\n",
    "    \"10000\": 1e-5,\n",
    "    \"20000\": 1e-6 \n",
    "}\n",
    "\n",
    "lr_decay_power = 4\n",
    "momentum = 0.9\n",
    "weight_decay = 0.005\n",
    "n_classes = 3\n",
    "n_boxes = 5\n",
    "partial_layer = 18\n",
    "\n",
    "start = time.time()\n",
    "imageNet_data = ImageNet_data(\"/home/ubuntu/sdcard/YOLOv2-master/XmlToTxt/water_bottle_img\",\n",
    "\"/home/ubuntu/sdcard/YOLOv2-master/XmlToTxt/water_bottle_bbox\", \"/home/ubuntu/sdcard/YOLOv2-master/XmlToTxt/images_list.txt\", n_classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing with serial_load backup weight\n"
     ]
    }
   ],
   "source": [
    "trained_model = Darknet19()\n",
    "serializers.load_npz(trained_weight_file, trained_model) # load saved model\n",
    "trained_model = Darknet19Predictor(trained_model)\n",
    "if initial_weight_file is None:\n",
    "    print(\"inititializing with the darknet19_448 weights\")\n",
    "    yolov2 = YOLOv2(n_classes=n_classes, n_boxes=n_boxes)\n",
    "    copy_conv_layer(trained_model.predictor, yolov2, range(1, partial_layer+1))\n",
    "    copy_bias_layer(trained_model.predictor, yolov2, range(1, partial_layer+1))\n",
    "    copy_bn_layer(trained_model.predictor, yolov2, range(1, partial_layer+1))\n",
    "    model = YOLOv2Predictor(yolov2)\n",
    "elif not (snapshot is None) :\n",
    "    yolov2 = YOLOv2(n_classes=n_classes, n_boxes=n_boxes)\n",
    "    model = YOLOv2Predictor(yolov2)\n",
    "    serializers.load_npz(snapshot, model)\n",
    "\n",
    "else :\n",
    "    print(\"initializing with serial_load backup weight\")\n",
    "    yolov2 = YOLOv2(n_classes=n_classes, n_boxes=n_boxes)\n",
    "    model = YOLOv2Predictor(yolov2)\n",
    "    serializers.load_hdf5(initial_weight_file, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predictor.train = True\n",
    "model.predictor.finetune = False  ####or True ??\n",
    "cuda.get_device(0).use()\n",
    "model.to_gpu()\n",
    "\n",
    "optimizer = optimizers.MomentumSGD(lr=learning_rate, momentum=momentum)\n",
    "optimizer.use_cleargrads()\n",
    "optimizer.setup(model)\n",
    "model.predictor.conv1.disable_update()\n",
    "model.predictor.conv2.disable_update()\n",
    "model.predictor.conv3.disable_update()\n",
    "model.predictor.conv4.disable_update()\n",
    "model.predictor.conv5.disable_update()\n",
    "model.predictor.conv6.disable_update()\n",
    "model.predictor.conv7.disable_update()\n",
    "model.predictor.conv8.disable_update()\n",
    "model.predictor.conv9.disable_update()\n",
    "model.predictor.conv10.disable_update()\n",
    "model.predictor.conv11.disable_update()\n",
    "model.predictor.conv12.disable_update()\n",
    "model.predictor.conv13.disable_update()\n",
    "model.predictor.conv14.disable_update()\n",
    "model.predictor.conv15.disable_update()\n",
    "model.predictor.conv16.disable_update()\n",
    "#model.predictor.conv17.disable_update()\n",
    "#model.predictor.conv18.disable_update()\n",
    "#model.predictor.conv19.disable_update()\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(weight_decay))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRNG = RandomState()\n",
    "\n",
    "\n",
    "transformDA = Compose([ [ColorJitter(brightness=0, contrast=0.7, saturation=0.7, hue=0, prob=0.5)],   # or write [ColorJitter(), None]\n",
    "        BoxesToCoords(relative=False),              \n",
    "           HorizontalFlip(),\n",
    "           VerticalFlip(),\n",
    "         RandomShift(),\n",
    "         Expand((1, 3), prob=0.5),\n",
    "        ObjectRandomCrop(),\n",
    "        RandomRotate(360),\n",
    "       Resize(448),   \n",
    "        CoordsToBoxesYOLO(),\n",
    "\n",
    "        #[SubtractMean(mean=VOC.MEAN)],\n",
    "        ], \n",
    "        PRNG, \n",
    "        mode='linear', \n",
    "        fillval=0, \n",
    "        outside_points='clamp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we transform data set for train\n"
     ]
    }
   ],
   "source": [
    "reporter = chainer.Reporter()\n",
    "\n",
    "\n",
    "#Create Train and test datset\n",
    "train, test = imageNet_data.train_val_test()\n",
    "\n",
    "#Transformed dataset, first creating function\n",
    "train_transf1 = partial(data_augmentation, arg=transformDA)\n",
    "print(\"we transform data set for train\")\n",
    "train_trans1 = TransformDataset(train, train_transf1)\n",
    "\n",
    "train_iter = iterators.SerialIterator(train_trans1, batch_size)\n",
    "#train_iter = iterators.SerialIterator(train, batch_size)\n",
    "test_iter = iterators.SerialIterator(test, batch_size, repeat=False, shuffle=True)\n",
    "test=1\n",
    "val_interval = (1 if test else 100000), 'epoch'\n",
    "log_interval = (10 if test else 1000), 'iteration'\n",
    "\n",
    "\n",
    "\n",
    "with chainer.using_config('debug', True):\n",
    "    # Set up a trainer\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, loss_func=model, converter=convert, device=0)\n",
    "    trainer = training.Trainer(updater, (100, 'epoch'), out=\"./backup/result\")\n",
    "updater.connect_trainer(trainer)\n",
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, converter=convert, device=0), trigger=val_interval)\n",
    "\n",
    "# Dump a computational graph from 'loss' variable at the first iteration\n",
    "# The \"main\" refers to the target link of the \"main\" optimizer.\n",
    "trainer.extend(extensions.dump_graph('main/loss'))\n",
    "\n",
    "# Take a snapshot at each epoch\n",
    "#trainer.extend(extensions.snapshot(), trigger=(args.epoch, 'epoch'))\n",
    "trainer.extend(extensions.snapshot(), trigger=(20, 'epoch'))\n",
    "trainer.extend(save_model,  trigger=(10, 'epoch'))\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer.extend(extensions.LogReport(trigger=log_interval))\n",
    "trainer.extend(extensions.PrintReport(\n",
    "      ['epoch', 'main/images', 'main/loss', 'validation/main/loss', 'elapsed_time']), trigger=log_interval)\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "# \"validation\" refers to the default name of the Evaluator extension.\n",
    "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "# either the updater or the evaluator.\n",
    "\n",
    "# Plot graph for loss for each epoch\n",
    "if extensions.PlotReport.available():\n",
    "    trainer.extend(extensions.PlotReport(\n",
    "        ['main/loss', 'validation/main/loss'],\n",
    "        x_key='epoch', file_name='loss.png'))\n",
    "else:\n",
    "    print('Warning: PlotReport is not available in your environment')\n",
    "# Print a progress bar to stdout\n",
    "trainer.extend(extensions.ProgressBar(update_interval=500))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\u001b[J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in main training loop: 'Compose' object is not iterable\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/chainer/training/trainer.py\", line 304, in run\n",
      "    update()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/chainer/training/updaters/standard_updater.py\", line 149, in update\n",
      "    self.update_core()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/chainer/training/updaters/standard_updater.py\", line 153, in update_core\n",
      "    batch = self._iterators['main'].next()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/chainer/iterators/serial_iterator.py\", line 81, in __next__\n",
      "    batch = [self.dataset[index] for index in self._order[i:i_end]]\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/chainer/iterators/serial_iterator.py\", line 81, in <listcomp>\n",
      "    batch = [self.dataset[index] for index in self._order[i:i_end]]\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/chainer/dataset/dataset_mixin.py\", line 67, in __getitem__\n",
      "    return self.get_example(index)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/chainer/datasets/transform_dataset.py\", line 47, in get_example\n",
      "    return self._transform(in_data)\n",
      "  File \"/media/sdmount/sdcard/YOLOv2-master/transforms/transforms.py\", line 1497, in data_augmentation\n",
      "    img, label= inputs\n",
      "Will finalize trainer extensions and updater before reraising the exception.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Compose' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cf2931af3426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Run the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 f.write('Will finalize trainer extensions and updater before '\n\u001b[1;32m    317\u001b[0m                         'reraising the exception.\\n')\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/chainer/training/updaters/standard_updater.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/chainer/training/updaters/standard_updater.py\u001b[0m in \u001b[0;36mupdate_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0min_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/chainer/iterators/serial_iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_order\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_end\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/chainer/iterators/serial_iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_order\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_end\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/chainer/dataset/dataset_mixin.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/chainer/datasets/transform_dataset.py\u001b[0m in \u001b[0;36mget_example\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0min_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/sdmount/sdcard/YOLOv2-master/transforms/transforms.py\u001b[0m in \u001b[0;36mdata_augmentation\u001b[0;34m(inputs, transformDA, train, show)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformDA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0;31m#we take only one input, the image already resized and the relative label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"img shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[0min_shape\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Compose' object is not iterable"
     ]
    }
   ],
   "source": [
    "with chainer.using_config('debug', True):\n",
    "    print(\"start\")\n",
    "    # Run the training\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serializers.save_hdf5(\"%s/yolov2_final.model\" % (\"./backup/result\"), model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
